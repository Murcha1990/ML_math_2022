{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Murcha1990/ML_math_2022/blob/main/Семинары/sem14-neural-networks.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![just add more layers](https://preview.redd.it/5193db0avbey.jpg?auto=webp&v=enabled&s=16e07a5eaabe7f85e191c16b49a8209d92336e65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "\n",
    "def load_mnist(dir: str = \"data\", cache: bool = True, flatten: bool = True):\n",
    "    train_data = MNIST(dir, train=True, download=True)\n",
    "    test_data = MNIST(dir, train=False, download=True)\n",
    "    \n",
    "    def load_fold(dataset, flatten=True):\n",
    "        X, y = [], []\n",
    "        for image, target in dataset:\n",
    "            X.append(np.asarray(image) / 255.)\n",
    "            y.append(target)\n",
    "        X = np.stack(X)\n",
    "        if flatten: \n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        return X, np.array(y)\n",
    "    \n",
    "    X_train, y_train = load_fold(train_data)\n",
    "    X_test, y_test = load_fold(test_data)\n",
    "\n",
    "    if not cache:\n",
    "        os.unlink(dir)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def plot(image: np.ndarray, target=None, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    ax.imshow(image.reshape(28, 28))\n",
    "    if target is not None:\n",
    "        ax.set_title(target)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Распознавание рукописных цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_mnist(flatten=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for img, tgt, ax in zip(X_train, y_train, axes.flatten()):\n",
    "    plot(img, tgt, ax)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max-nekrash/miniconda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "lr.fit(X_train.reshape(-1, 28 * 28), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9258"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test.reshape(-1, 28 * 28), y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети\n",
    "\n",
    "![](https://www.tibco.com/sites/tibco/files/media_entity/2021-05/neutral-network-diagram.svg)\n",
    "\n",
    "**Идея:** Линейная регрессия поверх линейной регрессии (поверх линейной регрессии).\n",
    "\n",
    "$$\n",
    "y = argmax\\bigg(f_3\\Big(f_2\\big(f_1(x)\\big)\\Big)\\bigg)\n",
    "$$\n",
    "\n",
    "\n",
    "*Если $f_i(x) = w_ix + b_i$, в чём проблема?* \n",
    "<details> \n",
    "  <summary>Спойлер</summary>\n",
    "    Композиция линейных функций - линейная функция.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции активации\n",
    "\n",
    "![activation_functions](https://www.researchgate.net/profile/Aaron-Stebner-2/publication/341310767/figure/fig7/AS:890211844255749@1589254451431/Common-activation-functions-in-artificial-neural-networks-NNs-that-introduce.ppm)\n",
    "\n",
    "Нужны, чтобы модель перестала быть линейной. Чаще всего берут $ReLU$ или $LeakyReLU$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце применим к выходам модели softmax, чтобы можно было интерпретировать предсказания как вероятности.\n",
    "\n",
    "**Итого:** модель имеет вид\n",
    "$$\n",
    "p = softmax(a) = softmax(w_3\\sigma(w_2\\sigma(w_1x + b_1) + b_2) + b_3)\n",
    "$$\n",
    "$a$ называются логитами"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ещё слои:\n",
    "* Dropout\n",
    "* Batch/layer normalization\n",
    "* Convolution\n",
    "* Recurrent layer\n",
    "* Attention\n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как это всё учить?\n",
    "\n",
    "**Нужно:**\n",
    "1. Лосс-функция (Кросс-энтропия)\n",
    "\n",
    "    Запишем кросс-энтропию, как функцию от логитов.\n",
    "\n",
    "    $$\n",
    "    loss = -\\sum_i\\log\\frac{\\exp(a_{correct, i})}{\\sum_k\\exp(a_{k,i})}\n",
    "    $$\n",
    "\n",
    "    *Это выражение можно упростить (как?)*\n",
    "\n",
    "    В таком виде оно более вычислительно стабильно и его проще дифферинцировать\n",
    "\n",
    "2. Метод оптимизации (SGD, Adam)\n",
    "\n",
    "    Решаем задачу $loss(f(X, w), y) \\to \\min_w$. Если $f$ - линейная регрессия, то знаем как. (Как?)\n",
    "\n",
    "    Чтобы применить такой же подход надо научиться брать производные.\n",
    "\n",
    "    Метод называется ~~производная композиции~~ backpropogation.\n",
    "\n",
    "\n",
    "**Итоговый метод:**\n",
    "\n",
    "`while True:`\n",
    "1. Применяем модель\n",
    "2. Считаем лосс\n",
    "3. Получаем градиенты\n",
    "4. Обновляем веса"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Даже это есть в sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "mlp.fit(X_train.reshape(-1, 28 * 28), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test.reshape(-1, 28 * 28), y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY часть\n",
    "\n",
    "Будем писать свою нейросеть. На `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/ (our ysda course).\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        # <your code. Try np.maximum>\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        # <your code>\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <x*W> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights with small random numbers. We use normal initialization, \n",
    "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <x*W> + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return #<your code here>\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = #<your code here>\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = #<your code here>\n",
    "        grad_biases = #<your code here>\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты на линейный слой\n",
    "\n",
    "Если тесты падают:\n",
    "\n",
    "<details> \n",
    "  <summary>Спойлер</summary>\n",
    "    <ul>\n",
    "        <li> Убедитесь, что градиенты для b - это сумма градиентов по батчу. Grad_output уже поделён на размер батча.\n",
    "        <li> Попробуйте сохранять переменные в аттрибут класса \"self.grad_w = grad_w\" или печатайте какие-то веса. Это помогает с дебагом.\n",
    "        <li> Если тесты всё расно падают, попробуйте их пропустить и приступить к обучению сети, возможно, эта ошибка не повлияет на само обучение.\n",
    "    </ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Dense(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the grads, we use gradients obtained via finite differences\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Dense(32,64,learning_rate=0)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test gradients w.r.t. params\n",
    "def compute_out_given_wb(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return l.forward(x)\n",
    "    \n",
    "def compute_grad_by_params(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    l.backward(x,np.ones([10,64]) / 10.)\n",
    "    return w - l.weights, b - l.biases\n",
    "    \n",
    "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
    "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
    "grad_w,grad_b = compute_grad_by_params(w,b)\n",
    "\n",
    "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test gradients w.r.t. params\n",
    "def compute_out_given_wb(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return l.forward(x)\n",
    "    \n",
    "def compute_grad_by_params(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    l.backward(x,np.ones([10,64]) / 10.)\n",
    "    return w - l.weights, b - l.biases\n",
    "    \n",
    "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
    "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
    "grad_w,grad_b = compute_grad_by_params(w,b)\n",
    "\n",
    "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лосс-функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n",
    "\n",
    "    xentropy = # <your code here>\n",
    "\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "\n",
    "    return # <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50) % 10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads, grads, rtol=1e-3, atol=0), \\\n",
    "    \"The reference implementation has just failed. Someone has just changed the rules of math.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус: Напишите Dropout слой"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собираем всё вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Dense(X_train.shape[1], 100))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(100, 200))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(200, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    # <your code here>\n",
    "\n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    Use network to predict the most likely class for each sample.\n",
    "    \"\"\"\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    You can estimate loss and loss_grad, obtaining dL / dy_pred\n",
    "    Then you can run layer.backward going from last layer to first, \n",
    "    propagating the gradient of input to previous layers.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    \n",
    "    # propagate gradients through network layers using .backward\n",
    "    # hint: start from last layer and move to earlier layers\n",
    "    <YOUR CODE>\n",
    "        \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        train(network, x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
